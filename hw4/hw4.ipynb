{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq_Tg5Xy-n8D",
        "outputId": "38adbf50-c007-46b1-d426-863cb6cb0dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 44 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 57.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=deed904bfbfbe5bbc0c1ae7e9cdcf52bb6f7bba53cb116947f171e439c2bd64a\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UYF1c9llFzpv"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SQLContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhfyRlaYGJFg",
        "outputId": "5cab6f0d-e8ad-443e-9479-b9d2c4ee973f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "sc = SparkContext()\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE5HCRHGU3Es",
        "outputId": "eaf6f5e5-1a08-4f39-a62b-bc0067a0e464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------------------+-----+------------+-------+------+------+-------------+\n",
            "|              id|        comment_text|toxic|severe_toxic|obscene|threat|insult|identity_hate|\n",
            "+----------------+--------------------+-----+------------+-------+------+------+-------------+\n",
            "|0000997932d777bf|Explanation\\nWhy ...|    0|           0|      0|     0|     0|            0|\n",
            "|000103f0d9cfb60f|D'aww! He matches...|    0|           0|      0|     0|     0|            0|\n",
            "|000113f07ec002fd|Hey man, I'm real...|    0|           0|      0|     0|     0|            0|\n",
            "|0001b41b1c6bb37e|\"\\nMore\\nI can't ...|    0|           0|      0|     0|     0|            0|\n",
            "|0001d958c54c6e35|You, sir, are my ...|    0|           0|      0|     0|     0|            0|\n",
            "+----------------+--------------------+-----+------------+-------+------+------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', multiline = True, escape = '\\\"').load('train.csv')\n",
        "data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gutrAeV2YsIn",
        "outputId": "683eb763-c437-464d-d258-e6f2797de9c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "labels = [column for column in data.columns if column not in [\"comment_text\",  \"id\"]]\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CetuBqCZRpw",
        "outputId": "11b21945-763f-4b22-ee09-93e7adfbe163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluation on tf&idf on toxic 0.8989244537273711\n",
            "evaluation on tf&idf on severe_toxic 0.9310365596798398\n",
            "evaluation on tf&idf on obscene 0.8925484864942721\n",
            "evaluation on tf&idf on threat 0.9394891486801633\n",
            "evaluation on tf&idf on insult 0.9036237458216981\n",
            "evaluation on tf&idf on identity_hate 0.901659678079029\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "regexTokenizer = RegexTokenizer(inputCol=\"comment_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
        "lr_models_tfidf = {}\n",
        "for label in labels:\n",
        "  label_stringIdx = StringIndexer(inputCol = label, outputCol = \"label\")\n",
        "  pipeline = Pipeline(stages=[regexTokenizer, hashingTF, idf, label_stringIdx])\n",
        "\n",
        "\n",
        "  pipelineFit = pipeline.fit(data)\n",
        "  dataset = pipelineFit.transform(data)\n",
        "\n",
        "  (trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n",
        "  lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "  lrModel = lr.fit(trainingData)\n",
        "  lr_models_tfidf[label] = lrModel\n",
        "\n",
        "  predictions = lrModel.transform(testData)\n",
        "  evaluator = BinaryClassificationEvaluator()\n",
        "  print(\"evaluation on tf&idf on \"+ label + \" \" + str(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTQw_4xQd3H1",
        "outputId": "818f25e8-0ce0-4e0f-c255-eea5f383ddf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluation hashingTF numFeatures = 1000 :0.8430641536054826\n",
            "evaluation hashingTF numFeatures = 2000 :0.851802063302561\n",
            "evaluation hashingTF numFeatures = 3000 :0.8789732832559787\n",
            "evaluation hashingTF numFeatures = 4000 :0.8772863304328277\n",
            "evaluation hashingTF numFeatures = 5000 :0.8878163331608615\n",
            "evaluation hashingTF numFeatures = 6000 :0.889256669267699\n",
            "evaluation hashingTF numFeatures = 7000 :0.8888300433317279\n",
            "evaluation hashingTF numFeatures = 8000 :0.8933010610529228\n",
            "evaluation hashingTF numFeatures = 9000 :0.8953694094029437\n",
            "evaluation hashingTF numFeatures = 10000 :0.8923546111029904\n",
            "evaluation hashingTF numFeatures = 11000 :0.8938495476296622\n",
            "evaluation hashingTF numFeatures = 12000 :0.8967176467706912\n",
            "evaluation hashingTF numFeatures = 13000 :0.8923978756964882\n",
            "evaluation hashingTF numFeatures = 14000 :0.8960668265949326\n",
            "evaluation hashingTF numFeatures = 15000 :0.8994409984922082\n",
            "evaluation hashingTF numFeatures = 16000 :0.9017589726976493\n",
            "evaluation hashingTF numFeatures = 17000 :0.904291836275138\n",
            "evaluation hashingTF numFeatures = 18000 :0.8969976398375485\n",
            "evaluation hashingTF numFeatures = 19000 :0.8996192982954604\n"
          ]
        }
      ],
      "source": [
        "label_stringIdx = StringIndexer(inputCol = \"toxic\", outputCol = \"label\")\n",
        "for n in range(1000, 20000, 1000):\n",
        "  hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=n)\n",
        "  pipeline = Pipeline(stages=[regexTokenizer, hashingTF, idf, label_stringIdx])\n",
        "  pipelineFit = pipeline.fit(data)\n",
        "  dataset = pipelineFit.transform(data)\n",
        "\n",
        "  (trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n",
        "  lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "  lrModel = lr.fit(trainingData)\n",
        "  predictions = lrModel.transform(testData)\n",
        "  evaluator = BinaryClassificationEvaluator()\n",
        "  print(\"evaluation hashingTF numFeatures = \"+ str(n) + \" :\" + str(evaluator.evaluate(predictions)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siUCqf_oCy6s",
        "outputId": "9adffd83-fa2b-4c1b-d7fa-a6a162059404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluation w2v on toxic 0.9446731069921979\n",
            "evaluation w2v on severe_toxic 0.9802581683406942\n",
            "evaluation w2v on obscene 0.9642604423735611\n",
            "evaluation w2v on threat 0.9517305950128626\n",
            "evaluation w2v on insult 0.9569818441459753\n",
            "evaluation w2v on identity_hate 0.952240737141214\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "w2v = Word2Vec(vectorSize=100, minCount=5, inputCol = \"words\", outputCol=\"features\")\n",
        "lr_models_w2v = {}\n",
        "for label in labels:\n",
        "  label_stringIdx = StringIndexer(inputCol = label, outputCol = \"label\")\n",
        "  pipeline = Pipeline(stages=[label_stringIdx, regexTokenizer, w2v])\n",
        "\n",
        "\n",
        "  pipelineFit = pipeline.fit(data)\n",
        "  dataset = pipelineFit.transform(data)\n",
        "\n",
        "  (trainingData, testData) = dataset.randomSplit([0.7, 0.3])\n",
        "  lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
        "  lrModel = lr.fit(trainingData)\n",
        "  lr_models_w2v[label] = lrModel\n",
        "\n",
        "  predictions = lrModel.transform(testData)\n",
        "  evaluator = BinaryClassificationEvaluator()\n",
        "  print(\"evaluation w2v on \"+ label + \" \" + str(evaluator.evaluate(predictions)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}